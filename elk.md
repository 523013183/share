## 环境

```
centos 7.8
一台项目服务器，一台日志服务器(根据需求可以将redis，elastic等部署到不同服务器)
```



## 配置yum 源

```shell
cd /etc/yum.repos.d && vi elastic.repo

[elastic-7.x]
name=Elastic repository for 7.x packages
baseurl=https://artifacts.elastic.co/packages/7.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md
```

## 注意

```
只有filebeat是在项目服务器上配置， 
其他的都在新服务器上配置.
看情况 中转redis是否要单独一台
```



## 安装filebeat

```shell
安装：yum install filebeat    #ps: 因为包的问题可能会中途失败，多次执行
配置： cd /etc/filebeat && vi filebeat.yml
			filebeat.inputs:
			- type: log
        enabled: true
        paths:
          - /var/www/logs/*.log #日志目录
        tags: ["logs"] #设置标签
        fields: #可根据自己的需求定义fields
    			log_source: logs #设置来源
    	- type: log
        enabled: true
        paths:
          - /var/www/procedure/*.log #日志目录
        tags: ["procedure"] #设置标签
        fields:
    			log_source: procedure #设置来源
 #----------------------------------	
      output.redis:
      hosts: ["192.168.2.70:6379"] #redis服务器
      password: "123456"
      db: 0
      timeout: 5
      datatype:list
      key: "log"
   
 #----------------------------------	  
启动：service filebeat start
停止：service filebeat stop



```

## 安装logstash

```shell
安装：yum install logstash  && yum install java-1.8.0-openjdk* -y
配置: cd /etc/logstash && vi logstash.yml
    path.config: /etc/logstash/conf.d
    http.host: "127.0.0.1"
    path.logs: /var/log/logstash
#----------------------------------  
	  # 多个，创建多个*.conf
    vi /etc/logstash/conf.d/nginx.conf
#----------------------------------
    # 从redis将数据取出
    input {
      redis {
        type => "log"
        host => "192.168.2.70"
        port => "6379"
        db => "0"
        password => "123456"
        data_type => "list"
        key => "log"
      }
    }


    # 格式化日志
    filter {
       grok {
            #match => [ "message","\[%{TIMESTAMP_ISO8601:logtime}\] %{WORD:env}\.(?<level>[A-Z]{4,5})\: %{GREEDYDATA:msg}}" ]
            match => [ "message","%{COMBINEDAPACHELOG}"]
            }
    }

    output {
            #过滤fields:log_source的日志
           if [fields][log_source] == "logs" {
             elasticsearch {
                  hosts => ["127.0.0.1:9200"]
                  index => "log"
                  user => "elastic"#设置了es密码就得开启
                  password => "123456"#设置了es密码就得开启
                }
           }
           if [fields][log_source] == "procedure" {
             elasticsearch {
                  hosts => ["127.0.0.1:9200"]
                  index => "procedure"
                  user => "elastic" #设置了es密码就得开启
                  password => "123456"#设置了es密码就得开启
                }
           }
    }
 
 配置service:
	 vi /etc/init.d/logstash #参考下方
	 vi /etc/default/logstash #参考下方
	 ln -s /etc/logstash/* /usr/share/logstash/config
	 chown -R logstash:logstash /usr/share/logstash/config/
	 
 service logstash start
 
#检测配置文件是否正常：
sudo -Hu logstash /usr/share/logstash/bin/logstash --path.settings=/etc/logstash -t
```

####  /etc/init.d/logstash 文件

```shell
#!/bin/sh
# Init script for logstash
# Maintained by
# Generated by pleaserun.
# Implemented based on LSB Core 3.1:
#   * Sections: 20.2, 20.3
#
### BEGIN INIT INFO
# Provides:          logstash
# Required-Start:    $remote_fs $syslog
# Required-Stop:     $remote_fs $syslog
# Default-Start:     2 3 4 5
# Default-Stop:      0 1 6
# Short-Description:
# Description:       logstash
### END INIT INFO

name=logstash
program=/usr/share/logstash/bin/logstash
pidfile="/var/run/logstash/$name.pid"

[ -r /etc/default/$name ] && . /etc/default/$name
[ -r /etc/sysconfig/$name ] && . /etc/sysconfig/$name

export KBN_PATH_CONF
export NODE_OPTIONS

[ -z "$nice" ] && nice=0

trace() {
  logger -t "/etc/init.d/logstash" "$@"
}

emit() {
  trace "$@"
  echo "$@"
}
start() {
  [ ! -d "/var/log/logstash/" ] && mkdir "/var/log/logstash/"
  chown "$user":"$group" "/var/log/logstash/"
  chmod 2750 "/var/log/logstash/"

  [ ! -d "/var/run/logstash/" ] && mkdir "/var/run/logstash/"
  chown "$user":"$group" "/var/run/logstash/"
  chmod 755 "/var/run/logstash/"

  chroot --userspec "$user":"$group" "$chroot" sh -c "

    cd \"$chdir\"
    exec \"$program\"
  " >> /var/log/logstash/logstash.stdout 2>> /var/log/logstash/logstash.stderr &

  # Generate the pidfile from here. If we instead made the forked process
  # generate it there will be a race condition between the pidfile writing
  # and a process possibly asking for status.
  echo $! > $pidfile

  emit "$name started"
  return 0
}
stop() {
  # Try a few times to kill TERM the program
  if status ; then
    pid=$(cat "$pidfile")
    trace "Killing $name (pid $pid) with SIGTERM"
    kill -TERM $pid
    # Wait for it to exit.
    for i in 1 2 3 4 5 ; do
      trace "Waiting $name (pid $pid) to die..."
      status || break
      sleep 1
    done
    if status ; then
      if [ "$KILL_ON_STOP_TIMEOUT" -eq 1 ] ; then
        trace "Timeout reached. Killing $name (pid $pid) with SIGKILL.  This may result in data loss."
        kill -KILL $pid
        emit "$name killed with SIGKILL."
      else
        emit "$name stop failed; still running."
      fi
    else
      emit "$name stopped."
    fi
  fi
}
status() {
  if [ -f "$pidfile" ] ; then
    pid=$(cat "$pidfile")
    if ps -p $pid > /dev/null 2> /dev/null ; then
      # process by this pid is running.
      # It may not be our pid, but that's what you get with just pidfiles.
      # TODO(sissel): Check if this process seems to be the same as the one we
      # expect. It'd be nice to use flock here, but flock uses fork, not exec,
      # so it makes it quite awkward to use in this case.
      return 0
    else
      return 2 # program is dead but pid file exists
    fi
  else
    return 3 # program is not running
  fi
}

force_stop() {
  if status ; then
    stop
    status && kill -KILL $(cat "$pidfile")
  fi
}

case "$1" in
  force-start|start|stop|force-stop|restart)
    trace "Attempting '$1' on logstash"
    ;;
esac

case "$1" in
  force-start)
    PRESTART=no
    exec "$0" start
    ;;
  start)
    status
    code=$?
    if [ $code -eq 0 ]; then
      emit "$name is already running"
      exit $code
    else
      start
      exit $?
    fi
    ;;
  stop) stop ;;
  force-stop) force_stop ;;
  status)
    status
    code=$?
    if [ $code -eq 0 ] ; then
      emit "$name is running"
    else
      emit "$name is not running"
    fi
    exit $code
    ;;
  restart)
  stop && start
    ;;
  *)
    echo "Usage: $SCRIPTNAME {start|force-start|stop|force-start|force-stop|status|restart}" >&2
    exit 3
  ;;
esac

exit $?
```

#### /etc/default/logstash 文件

```shell
user="logstash"
group="logstash"
chroot="/"
chdir="/"
nice=""


# If this is set to 1, then when `stop` is called, if the process has
# not exited within a reasonable time, SIGKILL will be sent next.
# The default behavior is to simply log a message "program stop failed; still running"
KILL_ON_STOP_TIMEOUT=0

BABEL_CACHE_PATH="/var/lib/logstash/optimize/.babel_register_cache.json"

KBN_PATH_CONF="/etc/logstash"
```



## 安装ES

```shell
安装：yum install -y elasticsearch
配置： vi /etc/elasticsearch/elasticsearch.yml
			node.name: Elstic
			# 监听地址，设置为127，只保持本机访问
			network.host: 127.0.0.1
			# 监听的端口
			http.port: 9200
			#----------------------
      #启动密码保护
      xpack.security.enabled: true
      xpack.security.authc.accept_default_password: true
     #---------------------------
     #外网访问
     #network.host: 127.0.0.1
     #http.host: 0.0.0.0
     #---------------------
     #初始化密码
      /usr/share/elasticsearch/bin/elasticsearch-setup-passwords interactive
    #设置密码  
    123456
service elasticsearch start
```

## 安装kibana

```shell
安装： yum install kibana
配置： vi /etc/kibana/kibana.yml

			server.port: 5601
			server.host: "127.0.0.1"
			elasticsearch.hosts: ["http://localhost:9200"]
			elasticsearch.username: "kibana"
			elasticsearch.password: "123456"
service kibana start
```

重启nginx

service nginx stop

service nginx start